{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df81fb74-6528-4a7e-b058-7bb7634a7160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ransomhouse\n",
    "\n",
    "def filter_recent_actions_from_json(json_data):\n",
    "    \"\"\"Filtra as entradas com actionDate dentro dos últimos 7 dias.\"\"\"\n",
    "\n",
    "    # Definindo os últimos 7 dias\n",
    "    hoje = datetime.now()\n",
    "    dias_7_atras = hoje - timedelta(days=7)\n",
    "\n",
    "    def is_within_last_7_days(date_str):\n",
    "        \"\"\"Verifica se a data está nos últimos 7 dias.\"\"\"\n",
    "        # Verificar se a string não está vazia\n",
    "        if not date_str:\n",
    "            return False\n",
    "        \n",
    "        # Converter a string da data para um objeto datetime\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, '%d/%m/%Y')\n",
    "            # Verificar se a data está dentro do intervalo\n",
    "            return dias_7_atras <= date_obj <= hoje\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    filtered_data = []\n",
    "    \n",
    "    for entry in json_data.get('data', []):\n",
    "        # Verifica se a chave 'actionDate' existe e se a data está nos últimos 7 dias\n",
    "        if 'actionDate' in entry and is_within_last_7_days(entry['actionDate']):\n",
    "            filtered_data.append({\n",
    "                'header': entry.get('header', ''),\n",
    "                'url': entry.get('url', ''),\n",
    "                'actionDate': entry.get('actionDate', '')\n",
    "            })\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "# Exemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    # Exemplo de JSON\n",
    "    json_example = json.loads(df_to_analize['html'][0])\n",
    "\n",
    "    # Filtrando os dados\n",
    "    filtered_data = filter_recent_actions_from_json(json_example)\n",
    "\n",
    "    # Exibindo o resultado\n",
    "    print(json.dumps(filtered_data, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a079ad8-41c7-48d3-bd31-59ca06c12007",
   "metadata": {},
   "outputs": [],
   "source": [
    "#monti\n",
    "\n",
    "dates_to_find = df_to_analize['datas_D-7'][1]\n",
    "html_content = df_to_analize['html'][1]\n",
    "def find_titles_by_dates(html_content, dates_to_find):\n",
    "    \n",
    "    # Criando um objeto BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Inicializando a lista de resultados\n",
    "    results = []\n",
    "\n",
    "    # Iterando na lista de datas e coletando títulos\n",
    "    for date in dates_to_find:\n",
    "        # Encontrando a div que contém a data específica\n",
    "        div = soup.find('div', class_='col-auto published', string=lambda text: text and date in text)\n",
    "        \n",
    "        if div:\n",
    "            # Encontrando o pai da div para obter o título\n",
    "            h5 = div.find_previous('h5')\n",
    "            title = h5.text.strip() if h5 else 'Título não encontrado'\n",
    "            results.append({'title': title, 'date': div.text.strip()})\n",
    "\n",
    "    return results\n",
    "\n",
    "# Chamando a função e exibindo os resultados\n",
    "results = find_titles_by_dates(html_content, dates_to_find)\n",
    "for result in results:\n",
    "    print(f\"Título: {result['title']}, Data: {result['date']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e830685-4828-471c-8535-0a0b00fc7db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# play\n",
    "\n",
    "html_content = df_to_analize['html'][2]\n",
    "\n",
    "def collect_recent_publications(html_content):\n",
    "    # Criando um objeto BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Data atual e data de corte para 7 dias atrás\n",
    "    current_date = datetime.now()\n",
    "    cutoff_date = current_date - timedelta(days=7)\n",
    "\n",
    "    # Coletando os dados\n",
    "    data_rows = []\n",
    "\n",
    "    for th in soup.find_all('th', class_='News'):\n",
    "        # Título é o texto do th, excluindo as tags filhas\n",
    "        title = th.contents[0].strip()  # Apenas o texto do <th> antes do <div>\n",
    "\n",
    "        # Encontrando o link\n",
    "        link_div = th.find('i', class_='link')\n",
    "        link = link_div.next_sibling.strip() if link_div and link_div.next_sibling else None\n",
    "        \n",
    "        # Encontrando a data de publicação\n",
    "        publication_date_div = th.find(string=lambda text: text and 'publication date:' in text)\n",
    "        publication_date = publication_date_div.strip().split(':')[-1].strip() if publication_date_div else None\n",
    "        \n",
    "        # Verificando se a data de publicação está nos últimos 7 dias\n",
    "        if publication_date:\n",
    "            pub_date = datetime.strptime(publication_date, '%Y-%m-%d')\n",
    "            if pub_date >= cutoff_date:  # Apenas incluir se a data de publicação for maior ou igual à data de corte\n",
    "                data_rows.append({'title': title, 'link': link, 'publication_date': publication_date})\n",
    "\n",
    "    return data_rows\n",
    "\n",
    "# Chamando a função e exibindo os resultados\n",
    "recent_publications = collect_recent_publications(html_content)\n",
    "for data in recent_publications:\n",
    "    print(f\"Título: {data['title']}, Link: {data['link']}, Data de Publicação: {data['publication_date']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f272cf-44d1-402f-8a13-c5b07dc0f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flocker\n",
    "\n",
    "html_content = df_to_analize['html'][3]\n",
    "dates_to_find = df_to_analize['datas_D-7'][3]\n",
    "\n",
    "def extract_data_from_html(html_content, dates_to_find):\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    results = []\n",
    "\n",
    "    # Iterando sobre os artigos\n",
    "    for article in soup.find_all('article'):\n",
    "        # Coletando a data de publicação\n",
    "        publication_date_span = article.find('span', itemprop='datePublished')\n",
    "        publication_date = publication_date_span.text.strip() if publication_date_span else None\n",
    "        \n",
    "        # Verificando se a data está na lista de datas a serem procuradas\n",
    "        if publication_date in dates_to_find:\n",
    "            # Coletando o título\n",
    "            title_tag = article.find('h2', itemprop='headline')\n",
    "            title = title_tag.text.strip() if title_tag else 'Título não encontrado'\n",
    "            \n",
    "            # Coletando a descrição e extraindo o site\n",
    "            description_tag = article.find('div', class_='ast-excerpt-container')\n",
    "            description = description_tag.text.strip() if description_tag else ''\n",
    "            \n",
    "            # Usando expressão regular para encontrar o site na descrição\n",
    "            site_match = re.search(r'\\b(?:[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}|(?:http|https):\\/\\/[^\\s]+|[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})\\b', description)\n",
    "            site = site_match.group(0) if site_match else 'Site não encontrado'\n",
    "            \n",
    "            # Adicionando o resultado à lista\n",
    "            results.append({\n",
    "                'title': title,\n",
    "                'publication_date': publication_date,\n",
    "                'site': site\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Chamando a função e exibindo os resultados\n",
    "results = extract_data_from_html(html_content, dates_to_find)\n",
    "for result in results:\n",
    "    print(f\"Título: {result['title']}, Data: {result['publication_date']}, Site: {result['site']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2306dbfc-5ba0-48fa-94e9-5a1c523ec4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handala\n",
    "\n",
    "html_content = df_to_analize['html'][4]\n",
    "target_dates  = df_to_analize['datas_D-7'][4]\n",
    "\n",
    "def extract_info_from_html(html_content, target_dates):\n",
    "    # Fazendo o parsing do HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Coletando informações\n",
    "    results = []\n",
    "    for li in soup.find_all('li', class_='wp-block-post'):\n",
    "        title = li.find('h2').find('a').text\n",
    "        date = li.find('time').get('datetime').split('T')[0]  # pegando apenas a data\n",
    "        description = li.find('p', class_='wp-block-post-excerpt__excerpt').text\n",
    "\n",
    "        # Verificando se a data está na lista de datas\n",
    "        if date in target_dates:\n",
    "            # Extraindo o site da descrição\n",
    "            site_match = re.search(r'\\b(?:https?://|www\\.)?([\\w.-]+(?:\\.[a-z]{2,}))\\b', description)\n",
    "            site = site_match.group(0) if site_match else None\n",
    "            \n",
    "            # Adicionando os resultados\n",
    "            results.append({\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'site': site\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Chamando a função e imprimindo os resultados\n",
    "results = extract_info_from_html(html_content, target_dates)\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b6de3e-fce4-4033-b903-816a98d9977a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
